{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get the data from the web\n",
    "\n",
    "The example below extracts data from the web. It uses BeautifulSoup to derive specific parts from the html protocol. In the case below it searches for the html class wikitable from the site https://en.wikipedia.org/wiki/Tilburg_Trappers\n",
    "\n",
    "    tables = soup.findAll(attrs={'class': re.compile(r\".*\\bwikitable\\b.*\")})\n",
    "\n",
    "selecting the first which is put in a dataframe. \n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Season  GP   W OTW OTL   L  Pts   GF   GA              Finish  \\\n",
      "0   2018/19  65  44   6   5  10  150  323  174  1st, Oberliga Nord   \n",
      "1   2017/18  59  43   8   3   5  110  265  129  1st, Oberliga Nord   \n",
      "2   2016/17  60  42   1   3  14  131  263  145  4th, Oberliga Nord   \n",
      "3   2015/16  57  40   3   1  11  130  284  119  2nd, Oberliga Nord   \n",
      "4   2014/15  24  17   2   0   5   38  126   53     2nd, Eredivisie   \n",
      "5   2013/14  36  30   2   4   4   64  216   64     1st, Eredivisie   \n",
      "6   2012/13  36  25   2   3   6   82  198   93     2nd, Eredivisie   \n",
      "7   2011/12  14  11   0   0   3   80   36   33  3rd, North Sea Cup   \n",
      "8   2010/11  28  19   0   1   8  120   70   58  2nd, North Sea Cup   \n",
      "9   2009/10  28  17   1   2   8  135   86   55     3rd, Eredivisie   \n",
      "10  2008/09  24  18   1   0   5  142   78   56     1st, Eredivisie   \n",
      "11  2007/08  24  20   1   0   3  132   58   62     1st, Eredivisie   \n",
      "12  2006/07  20  12   1   2   5   88   55   40     2nd, Eredivisie   \n",
      "13  2005/06  20   7   1   2  10   70   64   25     6th, Eredivisie   \n",
      "\n",
      "                                             Playoffs  \n",
      "0   Lost Oberliga Championship against EV Landshut...  \n",
      "1   Won Oberliga Championship against Deggendorfer...  \n",
      "2   Won Oberliga Championship against Tölzer Löwen...  \n",
      "3   Won Oberliga Championship against EHC Bayreuth...  \n",
      "4   Won Dutch Championship against UNIS Flyers Hee...  \n",
      "5   Won Dutch Championship against HYS The Hague (...  \n",
      "6                    Lost Finals to The Hague (0W-3L)  \n",
      "7                    Lost semi-finals to Geleen (0-3)  \n",
      "8                   Lost finals to HYS The Hague(1-4)  \n",
      "9                        Lost finals to Nijmegen(0-3)  \n",
      "10                     Lost finals to The Hague (2-3)  \n",
      "11  Won National Championship against Vadeko Flyer...  \n",
      "12  Won National Championship against Vadeko Flyer...  \n",
      "13                                    Did not qualify  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#### !/usr/bin/env python3\n",
    "\n",
    "__author__ = \"Fenna Feenstra\"\n",
    "\n",
    "import urllib.request, urllib.parse, urllib.error\n",
    "import ssl\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def hack_ssl():\n",
    "    \"\"\" ignores the certificate errors\"\"\"\n",
    "    ctx = ssl.create_default_context()\n",
    "    ctx.check_hostname = False\n",
    "    ctx.verify_mode = ssl.CERT_NONE\n",
    "    return ctx\n",
    "\n",
    "\n",
    "def open_url(url):\n",
    "    \"\"\" opens url\"\"\"\n",
    "    ctx = hack_ssl()\n",
    "    html = urllib.request.urlopen(url, context=ctx).read()\n",
    "    return html\n",
    "    \n",
    "\n",
    "def fetch_tables(html):\n",
    "    \"\"\" reads html file as a big string and cleans the html file to make it\n",
    "        more readable. input: html, output: tables\n",
    "    \"\"\"\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    tables = soup.findAll(attrs={'class': re.compile(r\".*\\bwikitable\\b.*\")})\n",
    "    return tables[0]\n",
    "\n",
    "\n",
    "def table_df(table):\n",
    "    \"\"\"parses the html table to a pandas dataframe\"\"\"\n",
    "    #fetch dimensions\n",
    "    l = len(table.find_all('tr')) \n",
    "    w = len(table.find_all('tr')[0].find_all('td'))\n",
    "    matrix = [['' for i in range(0,w)] for i in range(0,l)]\n",
    "    #fetch content\n",
    "    for i, row in enumerate(table.find_all('tr')):\n",
    "        for j, column in enumerate(row.find_all('td')):        \n",
    "            matrix[i][j]=column.get_text().strip()\n",
    "    #put in df making first row the header\n",
    "    df = pd.DataFrame(matrix[1:], columns = matrix[0])\n",
    "    return df\n",
    "\n",
    "\n",
    "def main():\n",
    "    html = open_url('https://en.wikipedia.org/wiki/Tilburg_Trappers')\n",
    "    t = fetch_tables(html)\n",
    "    df = table_df(t)\n",
    "    print(df)\n",
    "    return 0\n",
    "\n",
    "    \n",
    "main()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment week 01\n",
    "\n",
    "Check the site https://www.knmi.nl/nederland-nu/seismologie/aardbevingen\n",
    "\n",
    "On this site you find a table of seismologic activities with the following fields.\n",
    "\n",
    "    Analysis\n",
    "    Date and time (UTC)\n",
    "    Place\n",
    "    Magnitude\n",
    "    Depth (km)\n",
    "    Type of earthquacke\n",
    "    Details\n",
    "\n",
    "Your job is to fetch the table with the last 15 seismologic activities. Only select the `'Date and Time (UTC)', 'Place', 'Magnitude', 'Depth (km)', 'Type earthquacke'` columns and put it in a pandas dataframe. See example below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<ssl.SSLContext object at 0x7efc769e0868>\n",
      "aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa\n",
      "[]\n",
      "aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa\n"
     ]
    }
   ],
   "source": [
    "#### !/usr/bin/env python3\n",
    "\n",
    "__author__ = \"Fenna Feenstra\"\n",
    "\n",
    "import urllib.request, urllib.parse, urllib.error\n",
    "import ssl\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "def hack_ssl():\n",
    "    \"\"\" ignores the certificate errors\"\"\"\n",
    "    ctx = ssl.create_default_context()\n",
    "    ctx.check_hostname = False\n",
    "    ctx.verify_mode = ssl.CERT_NONE\n",
    "    print(ctx)\n",
    "    return ctx\n",
    "\n",
    "\n",
    "def open_url(url):\n",
    "    \"\"\" opens url\"\"\"\n",
    "    ctx = hack_ssl()\n",
    "    html = urllib.request.urlopen(url, context=ctx).read()\n",
    "    #print(html)\n",
    "    return html\n",
    "    \n",
    "\n",
    "def fetch_tables(html):\n",
    "    \"\"\" reads html file as a big string and cleans the html file to make it\n",
    "        more readable. input: html, output: tables\n",
    "    \"\"\"\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    tables = soup.findAll(attrs={'class': re.compile(r\".*\\bwikitable\\b.*\")})\n",
    "    print(\"aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa\")\n",
    "    print(tables)\n",
    "    print(\"aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa\")\n",
    "    #return tables[0]\n",
    "\n",
    "\n",
    "def table_df(table):\n",
    "    \"\"\"parses the html table to a pandas dataframe\"\"\"\n",
    "    #fetch dimensions\n",
    "    l = len(table.find_all('tr')) \n",
    "    w = len(table.find_all('tr')[0].find_all('td'))\n",
    "    matrix = [['' for i in range(0,w)] for i in range(0,l)]\n",
    "    #fetch content\n",
    "    for i, row in enumerate(table.find_all('tr')):\n",
    "        for j, column in enumerate(row.find_all('td')):        \n",
    "            matrix[i][j]=column.get_text().strip()\n",
    "    #put in df making first row the header\n",
    "    df = pd.DataFrame(matrix[1:], columns = matrix[0])\n",
    "    return df\n",
    "\n",
    "\n",
    "def main():\n",
    "    #html = open_url('https://en.wikipedia.org/wiki/Tilburg_Trappers')\n",
    "    html = open_url('https://www.knmi.nl/nederland-nu/seismologie/aardbevingen')\n",
    "    t = fetch_tables(html)\n",
    "    #df = table_df(t)\n",
    "    #print(df)\n",
    "    #return 0\n",
    "\n",
    "    \n",
    "main()\n",
    "    "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "    Date and Time (UTC)           Place Magnitude  Depth (km) Type earthquacke\n",
    "0   2019-04-16 02:15:07       Harkstede       0.9         3.0     Geïnduceerd\n",
    "1   2019-04-15 07:03:43      Noordwolde       1.6         3.0     Geïnduceerd\n",
    "2   2019-04-10 04:40:10       Zuidbroek       1.4         3.0     Geïnduceerd\n",
    "3   2019-04-09 19:26:30           Spijk       1.4         3.0     Geïnduceerd\n",
    "4   2019-04-07 07:07:33     Garrelsweer       0.9         3.0     Geïnduceerd\n",
    "5   2019-03-30 01:51:36      Appingedam       0.5         3.0     Geïnduceerd\n",
    "6   2019-03-29 13:37:20         Kantens       1.5         3.0     Geïnduceer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is not clean, since it might contain data from Belgium or Germany. Remove that data. Further more we would like some more insight in the statistics of the data. Calculate the minimum, maximum, mean and standard deviation of the Magnitude using NumPy. Plot the magnitudes choosing a visualisation of your choice."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
