{
  "cells": [
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "# Web scraping\n\nPython has many libraries for reading and writing data in HMTL and XML formats. Examples include `lxml` (http://lxml.de), `Beautiful Soup`, `html5lib`. While lxml is much faster Beautiful Soup and html5lib can handle malformed HTML or XML files. `pandas` has a builtin function `read_html` wich uses Beautiful Soup (https://www.crummy.com/software/BeautifulSoup/) and lxml under the hood. Since we might need to fix or parameterize things 'under the hood' we will work with a Beautiful Soup study case in this course as well. Later on in this programming 1 course we work with JSON, web API's, *hiearchal data formats* like HDF5 files and SQL databases. "
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Before we start working with web scraper libraries we need to install them. If the system does not have the libraries lxml, beautifulsoup4 and html5lib we can install them in a virtual environment on our system."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "virtualenv -p /usr/bin/python3 venv\nsource venv/bin/activate\n\n#install tools\npip3 install beautifulsoup4\npip3 install html5lib\npip3 install lxml",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Scraping HTML"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Now let us try to scrape the web via the builtin pandas.read_html. Most of the time this does not work since there are some issues like certification"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "import pandas as pd\nurl = 'https://en.wikipedia.org/wiki/Tilburg_Trappers'\ntables = pd.read_html(url)\ntables[0]",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Indeed we get an `URLError: <urlopen error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed (_ssl.c:777)>` error. "
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "We need to implement a work around with BeautifulSoup directly. First we open the url with the library urllib.request.urlopen() parsing the url and the 'do not verify the certificate mode'. Basically this is the wget function in the terminal. \n"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "import urllib.request, urllib.parse, urllib.error\nimport ssl\nfrom bs4 import BeautifulSoup\nimport re\nimport pandas as pd\n\n\ndef hack_ssl():\n    \"\"\" ignores the certificate errors\"\"\"\n    ctx = ssl.create_default_context()\n    ctx.check_hostname = False\n    ctx.verify_mode = ssl.CERT_NONE\n    return ctx\n\n\ndef open_url(url):\n    \"\"\" opens url\"\"\"\n    ctx = hack_ssl()\n    html = urllib.request.urlopen(url, context=ctx).read()\n    return html\n    ",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "The pd.read_html fetches all the tables directly. We need to add another function to extract the tables from the html. When we inspect the html we can see that the table of interest is a wikitable, not a table. So we use a regex to find the wikitables.\n\n"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "def fetch_tables(html):\n    \"\"\" reads html file as a big string and cleans the html file to make it\n        more readable. input: html, output: tables\n    \"\"\"\n    soup = BeautifulSoup(html, 'html.parser')\n    tables = soup.findAll(attrs={'class': re.compile(r\".*\\bwikitable\\b.*\")})\n    return tables[0]",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "We now extracted the first table. "
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "def main():\n    html = open_url('https://en.wikipedia.org/wiki/Tilburg_Trappers')\n    t = fetch_tables(html)\n    print(t)\n    return 0\n\n    \nmain()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "in the html documentation https://www.w3schools.com/ we can see that `<table>` defines a table, `<tr>` defines a row and `<td>` defines a cell. We can use this to fetch the rows and columns and put them in a dataframe. Since the first row 'tr' is used as a header (correct html would use `<th>`) we can use the first row as an index and we need to consider the following `<tr>'`s (`matrix[1:]`) as data. This is exactly the reason why we need to inspect the html code first. Most of the time it is crappy :-("
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "def table_df(table):\n    \"\"\"parses the html table to a pandas dataframe\"\"\"\n    #fetch dimensions\n    l = len(table.find_all('tr')) \n    w = len(table.find_all('tr')[0].find_all('td'))\n    matrix = [['' for i in range(0,w)] for i in range(0,l)]\n    #fetch content\n    for i, row in enumerate(table.find_all('tr')):\n        for j, column in enumerate(row.find_all('td')):        \n            matrix[i][j]=column.get_text().strip()\n    #put in df making first row the header\n    df = pd.DataFrame(matrix[1:], columns = matrix[0])\n    return df",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Chaning the main function will now print the dataframe"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "def main():\n    html = open_url('https://en.wikipedia.org/wiki/Tilburg_Trappers')\n    t = fetch_tables(html)\n    df = table_df(t)\n    print(df)\n    return 0\n\n    \nmain()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Finally we have a dataframe to work with. We can now use NumPy to conduct the analysis and to perform calculations. Remember that the html is text, so we might need to transfer data into another data format. More about NumPy and pandas in the next lecture. "
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Scraping XML"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "XML is another common structured data format supporting hierarchal nested data with metadata. XML and HTML are structured simular but XML is more general. An example of XML you find below"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "<people>\n  <person>\n      <name>Fenna</name>\n      <address>Maluslaan 116</address>\n  </person>\n  <person>\n      <name>Kees</name>\n      <address>Onbekend</address>\n    </person>\n</people>"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "We can fetch the xml tree by open the url and read the data and decode the data. "
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "import urllib.request, urllib.parse, urllib.error\nimport xml.etree.ElementTree as ET\nimport ssl\n\n\n# Ignore SSL certificate errors\nctx = ssl.create_default_context()\nctx.check_hostname = False\nctx.verify_mode = ssl.CERT_NONE\n\nurl = 'https://bioinf.nl/~fennaf/DSLS/plants.xml'\n#url = 'http://www.phyloxml.org/examples/apaf.xml'\nprint('Retrieving', url)\nuh = urllib.request.urlopen(url, context=ctx)\n\ndata = uh.read()\nprint('Retrieved', len(data), 'characters')\nprint(data.decode())\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "If we need specific information we can use element tree to fetch that data"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "import xml.etree.ElementTree as ET\n\ndata = '''\n<person>\n  <name>Fenna</name>\n  <phone type=\"intl\">\n    +31646080034\n  </phone>\n  <email hide=\"yes\" />\n</person>'''\n\ntree = ET.fromstring(data)\nprint('Name:', tree.find('name').text)\nprint('Attr:', tree.find('email').get('hide'))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "import urllib.request, urllib.parse, urllib.error\nimport xml.etree.ElementTree as ET\nimport ssl\n\n\n# Ignore SSL certificate errors\nctx = ssl.create_default_context()\nctx.check_hostname = False\nctx.verify_mode = ssl.CERT_NONE\n\nurl = 'https://bioinf.nl/~fennaf/DSLS/plants.xml'\nprint('Retrieving', url)\nuh = urllib.request.urlopen(url, context=ctx)\n\ndata = uh.read()\ntree = ET.fromstring(data)\nfor child in tree:\n    print('\\n')\n    for element in child:\n        print(element.tag, element.text)\n",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    },
    "language_info": {
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "name": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.4",
      "file_extension": ".py",
      "codemirror_mode": {
        "version": 3,
        "name": "ipython"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}